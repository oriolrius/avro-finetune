# Hugging Face API Token
# Required for accessing gated models like Phi-3
# 
# How to obtain your token:
# 1. Create a Hugging Face account at https://huggingface.co/join
# 2. Go to Settings > Access Tokens at https://huggingface.co/settings/tokens
# 3. Click "New token" and create a token with "read" permissions
# 4. Copy the token (starts with "hf_")
# 5. Replace the placeholder below with your actual token
#
# IMPORTANT: 
# - Never commit your actual token to version control
# - Keep this file as .env.example in the repository
# - Copy it to .env locally and add your real token there
# - The .env file is gitignored for security

HF_TOKEN=hf_your_token_here

# Model Configuration
MODEL_ID=microsoft/Phi-3-mini-4k-instruct
DATASET_PATH=dataset_minimal.jsonl
OUTPUT_DIR=./avro-phi3-adapters

# LoRA Configuration
LORA_RANK=32
LORA_ALPHA=64
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# Training Configuration
TRAIN_BATCH_SIZE=2
GRADIENT_ACCUMULATION_STEPS=2
LEARNING_RATE=5e-5
NUM_TRAIN_EPOCHS=20
MAX_LENGTH=2048
WARMUP_RATIO=0.1
WEIGHT_DECAY=0.01
LOGGING_STEPS=5
SAVE_STRATEGY=epoch
SAVE_TOTAL_LIMIT=3

# Hardware Configuration
USE_FLASH_ATTENTION=true
USE_FP16=true
LOAD_IN_4BIT=true
BNB_4BIT_COMPUTE_DTYPE=bfloat16
BNB_4BIT_QUANT_TYPE=nf4
BNB_4BIT_USE_DOUBLE_QUANT=true

# Export Configuration (new in v3.0)
# Automatically export models for deployment after training
EXPORT_VLLM=false          # Export for vLLM inference server
EXPORT_OLLAMA=false        # Export for Ollama local deployment
AUTO_MERGE=false           # Automatically merge and export (if false, just prints instructions)