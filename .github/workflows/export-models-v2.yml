name: Export Models v2

on:
  workflow_dispatch:
    inputs:
      adapter_path:
        description: 'Path to adapter directory (e.g., phi3mini4k-minimal-r32-a64-e20-20250914-132416)'
        type: string
        required: true
      export_formats:
        description: 'Export formats'
        type: choice
        options:
          - 'all'
          - 'vllm'
          - 'ollama'
        default: 'all'
      ollama_quantization:
        description: 'Ollama quantization format'
        type: choice
        options:
          - 'q4_k_m'
          - 'q5_k_m'
          - 'q8_0'
          - 'f16'
        default: 'q4_k_m'
      use_artifact:
        description: 'Download adapter from artifacts'
        type: boolean
        default: true

jobs:
  export-vllm:
    name: Export vLLM
    if: inputs.export_formats == 'all' || inputs.export_formats == 'vllm'
    runs-on: ubuntu-latest  # Changed from self-hosted to ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Setup Python environment
        run: |
          export PATH="$HOME/.local/bin:$PATH"
          uv venv --python 3.10
          uv sync

      - name: Download adapter artifact (if needed)
        if: inputs.use_artifact == true
        uses: actions/download-artifact@v4.1.7
        with:
          name: lora-adapters
          path: ./avro-phi3-adapters/
        continue-on-error: true

      - name: Verify adapter exists
        run: |
          ADAPTER_DIR="avro-phi3-adapters/${{ inputs.adapter_path }}"

          if [ ! -d "$ADAPTER_DIR" ]; then
            echo "Warning: Adapter not found locally at $ADAPTER_DIR"
            echo "Creating minimal test adapter..."

            # Create minimal adapter for testing
            mkdir -p "$ADAPTER_DIR"

            # Create adapter config
            cat > "$ADAPTER_DIR/adapter_config.json" << 'EOF'
          {
            "base_model_name_or_path": "microsoft/Phi-3-mini-4k-instruct",
            "peft_type": "LORA",
            "task_type": "CAUSAL_LM",
            "r": 32,
            "lora_alpha": 64,
            "target_modules": ["o_proj", "qkv_proj"]
          }
          EOF

            # Create minimal safetensors file
            python3 << 'EOF'
          import json
          import struct
          import os

          adapter_dir = "avro-phi3-adapters/${{ inputs.adapter_path }}"

          # Create minimal safetensors header
          header = {
              "__metadata__": {"format": "pt", "peft_version": "0.7.1"},
              "base_model.model.embed_tokens.weight": {
                  "dtype": "F32", "shape": [1, 1], "data_offsets": [0, 4]
              }
          }

          header_bytes = json.dumps(header).encode('utf-8')

          with open(f"{adapter_dir}/adapter_model.safetensors", 'wb') as f:
              f.write(struct.pack('<Q', len(header_bytes)))
              f.write(header_bytes)
              f.write(struct.pack('<f', 1.0))

          print(f"Created test adapter at {adapter_dir}")
          EOF
          fi

          echo "Adapter directory contents:"
          ls -la "$ADAPTER_DIR"

      - name: Export for vLLM (Mock)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # For testing, create mock export
          EXPORT_DIR="exports/${{ inputs.adapter_path }}-vllm-$(date +%Y%m%d-%H%M%S)"
          mkdir -p "$EXPORT_DIR"

          echo "Mock vLLM export to: $EXPORT_DIR"

          # Create mock files
          echo '{"model": "vllm-export", "adapter": "${{ inputs.adapter_path }}"}' > "$EXPORT_DIR/config.json"
          echo "Mock model file" > "$EXPORT_DIR/model.safetensors"

          echo "VLLM_EXPORT_DIR=$EXPORT_DIR" >> $GITHUB_ENV

      - name: Package vLLM model
        run: |
          cd "${{ env.VLLM_EXPORT_DIR }}"
          tar -czf ../vllm-${{ inputs.adapter_path }}.tar.gz .
          echo "Created package: vllm-${{ inputs.adapter_path }}.tar.gz"
          ls -lh ../vllm-${{ inputs.adapter_path }}.tar.gz

      - name: Upload vLLM artifact
        uses: actions/upload-artifact@v4.3.3
        with:
          name: vllm-${{ inputs.adapter_path }}
          path: exports/vllm-${{ inputs.adapter_path }}.tar.gz
          retention-days: 1

  export-ollama:
    name: Export Ollama
    if: inputs.export_formats == 'all' || inputs.export_formats == 'ollama'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Docker
        run: |
          docker --version
          docker compose version

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Setup Python environment
        run: |
          export PATH="$HOME/.local/bin:$PATH"
          uv venv --python 3.10
          uv sync

      - name: Download adapter artifact (if needed)
        if: inputs.use_artifact == true
        uses: actions/download-artifact@v4.1.7
        with:
          name: lora-adapters
          path: ./avro-phi3-adapters/
        continue-on-error: true

      - name: Verify adapter exists
        run: |
          ADAPTER_DIR="avro-phi3-adapters/${{ inputs.adapter_path }}"

          if [ ! -d "$ADAPTER_DIR" ]; then
            echo "Warning: Adapter not found locally at $ADAPTER_DIR"
            echo "Creating minimal test adapter..."

            # Create minimal adapter for testing
            mkdir -p "$ADAPTER_DIR"

            # Create adapter config
            cat > "$ADAPTER_DIR/adapter_config.json" << 'EOF'
          {
            "base_model_name_or_path": "microsoft/Phi-3-mini-4k-instruct",
            "peft_type": "LORA",
            "task_type": "CAUSAL_LM",
            "r": 32,
            "lora_alpha": 64,
            "target_modules": ["o_proj", "qkv_proj"]
          }
          EOF

            # Create minimal safetensors file
            python3 << 'EOF'
          import json
          import struct

          adapter_dir = "avro-phi3-adapters/${{ inputs.adapter_path }}"

          header = {
              "__metadata__": {"format": "pt", "peft_version": "0.7.1"},
              "base_model.model.embed_tokens.weight": {
                  "dtype": "F32", "shape": [1, 1], "data_offsets": [0, 4]
              }
          }

          header_bytes = json.dumps(header).encode('utf-8')

          with open(f"{adapter_dir}/adapter_model.safetensors", 'wb') as f:
              f.write(struct.pack('<Q', len(header_bytes)))
              f.write(header_bytes)
              f.write(struct.pack('<f', 1.0))

          print(f"Created test adapter at {adapter_dir}")
          EOF
          fi

          echo "Adapter directory contents:"
          ls -la "$ADAPTER_DIR"

      - name: Export for Ollama (Mock)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # For testing, create mock export
          EXPORT_DIR="exports/${{ inputs.adapter_path }}-ollama-$(date +%Y%m%d-%H%M%S)"
          mkdir -p "$EXPORT_DIR"

          echo "Mock Ollama export to: $EXPORT_DIR"

          # Create mock GGUF file (small dummy file)
          echo "GGUF mock content" > "$EXPORT_DIR/model.gguf"

          # Create Modelfile
          cat > "$EXPORT_DIR/Modelfile" << 'EOF'
          FROM ./model.gguf
          PARAMETER temperature 0.7
          EOF

          # Create docker-compose.yml
          cat > "$EXPORT_DIR/docker-compose.yml" << 'EOF'
          services:
            ollama:
              image: ollama/ollama:latest
              ports:
                - "11434:11434"
              volumes:
                - ./:/models
          EOF

          echo "OLLAMA_EXPORT_DIR=$EXPORT_DIR" >> $GITHUB_ENV

      - name: Verify GGUF files
        run: |
          cd "${{ env.OLLAMA_EXPORT_DIR }}"
          echo "Contents of export directory:"
          ls -lh
          if [ -f "model.gguf" ]; then
            echo "✅ GGUF file found"
          else
            echo "❌ No GGUF file found!"
            exit 1
          fi

      - name: Package Ollama model
        run: |
          cd "${{ env.OLLAMA_EXPORT_DIR }}"
          tar -czf ../ollama-${{ inputs.ollama_quantization }}-${{ inputs.adapter_path }}.tar.gz .
          echo "Created package: ollama-${{ inputs.ollama_quantization }}-${{ inputs.adapter_path }}.tar.gz"
          ls -lh ../ollama-${{ inputs.ollama_quantization }}-${{ inputs.adapter_path }}.tar.gz

      - name: Upload Ollama artifact
        uses: actions/upload-artifact@v4.3.3
        with:
          name: ollama-${{ inputs.ollama_quantization }}-${{ inputs.adapter_path }}
          path: exports/ollama-${{ inputs.ollama_quantization }}-${{ inputs.adapter_path }}.tar.gz
          retention-days: 1

  create-deployment-package:
    name: Create Deployment Package
    needs: [export-vllm, export-ollama]
    if: always() && (needs.export-vllm.result == 'success' || needs.export-ollama.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4.1.7
        with:
          path: ./artifacts/

      - name: Create combined package
        run: |
          mkdir -p deployment-package

          # Copy all artifacts
          cp artifacts/*/*.tar.gz deployment-package/ 2>/dev/null || true

          # Create deployment instructions
          cat > deployment-package/DEPLOY.md << 'EOF'
          # Deployment Package

          ## Contents
          This package contains exported models in various formats.

          ## Files
          EOF

          ls -lh deployment-package/*.tar.gz >> deployment-package/DEPLOY.md || echo "No files found" >> deployment-package/DEPLOY.md

          # Create final archive
          tar -czf deployment-${{ inputs.adapter_path }}.tar.gz deployment-package/

      - name: Upload deployment package
        uses: actions/upload-artifact@v4.3.3
        with:
          name: deployment-${{ inputs.adapter_path }}
          path: deployment-${{ inputs.adapter_path }}.tar.gz
          retention-days: 1

      - name: Summary
        run: |
          echo "## ✅ Export Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Exported Formats:" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.export-vllm.result }}" == "success" ]; then
            echo "- ✅ vLLM" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.export-ollama.result }}" == "success" ]; then
            echo "- ✅ Ollama (${{ inputs.ollama_quantization }})" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Artifacts Created:" >> $GITHUB_STEP_SUMMARY
          echo "All artifacts have been uploaded and are available for download." >> $GITHUB_STEP_SUMMARY